import logging
import pandas as pd
import pendulum
from airflow.decorators import dag, task
from airflow.providers.postgres.hooks.postgres import PostgresHook
from sqlalchemy import create_engine
from sqlalchemy import text
from steps.CleanData import EmptyRR, Dublikates, EmptyCells, Vibros, Vibros2
from steps.messages import send_telegram_success_message, send_telegram_failure_message


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dag(
    schedule_interval="@once",
    start_date=pendulum.datetime(2024, 1, 1, tz="UTC"),
    catchup=False,
    tags=["real_estate_cleaning"],
    on_success_callback=send_telegram_success_message,
    on_failure_callback=send_telegram_failure_message,   
)
def real_estate_data_cleaning():

    @task
    def extract_data() -> pd.DataFrame:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü—ã real_estate_dataset."""
        hook = PostgresHook('destination_db')
        try:
            conn = hook.get_conn()
            logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise

        query = "SELECT * FROM real_estate_dataset"
        df = pd.read_sql(query, conn)
        logger.info("üì• –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å—Ç—Ä–æ–∫: %d", len(df))
        return df

    @task
    def clean_data(df: pd.DataFrame) -> pd.DataFrame:
        """–û—á–∏—Å—Ç–∫–∞ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö."""
        cols_b1 = df.shape[1]
        initial_lenb1 = len(df)
    
        df = EmptyRR(df)
        df = Dublikates(df)
        df = EmptyCells(df)
        df = Vibros2(df)
        #df = Vibros(df)
    
        cols_after1 = df.shape[1]
        logger.info("üóëÔ∏è –í—Å–µ–≥–æ –£–¥–∞–ª–µ–Ω–æ —Å—Ç–æ–ª–±—Ü–æ–≤: %d", cols_b1 - cols_after1)
        logger.info("üóëÔ∏è –í—Å–µ–≥–æ –£–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫: %d", initial_lenb1 - len(df))
        logger.info("üîß –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω—ã")
        return df

    @task
    def load_cleaned_data(df: pd.DataFrame):
        """–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å —Ç–∞–±–ª–∏—Ü—ã real_estate_dataset_clean –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        try:
            logger.info("üîÑ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö (–ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å —Ç–∞–±–ª–∏—Ü—ã)")

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ PostgresHook
            hook = PostgresHook(postgres_conn_id='destination_db')
            conn = hook.get_connection('destination_db')
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            connection_uri = f"postgresql+psycopg2://{conn.login}:{conn.password}@{conn.host}:{conn.port}/{conn.schema}"
            engine = create_engine(connection_uri)

            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã (–µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
            with engine.begin() as connection:
                connection.execute(text("DROP TABLE IF EXISTS real_estate_dataset_clean CASCADE"))
                logger.info("üóëÔ∏è –°—Ç–∞—Ä–∞—è —Ç–∞–±–ª–∏—Ü–∞ —É–¥–∞–ª–µ–Ω–∞ (–µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–ª–∞)")

                # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                df.to_sql(
                    name='real_estate_dataset_clean',
                    con=connection,
                    if_exists='fail',  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ —Ç–∞–±–ª–∏—Ü—ã –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                    index=False,
                    method='multi',
                    chunksize=1000
                )
                logger.info("‚úÖ –ù–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞ –∏ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
            raise

    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á –≤ DAG
    df = extract_data()
    cleaned_df = clean_data(df)
    load_cleaned_data(cleaned_df)

# –°–æ–∑–¥–∞–Ω–∏–µ DAG
dag = real_estate_data_cleaning()