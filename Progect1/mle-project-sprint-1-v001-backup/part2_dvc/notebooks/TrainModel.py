import pandas as pd
import os
import joblib
import logging
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from category_encoders import CatBoostEncoder
from catboost import CatBoostRegressor
from dotenv import load_dotenv
from sqlalchemy import create_engine


# –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ —Ä–∞–Ω–µ–µ –Ω–µ –∑–∞–¥–∞–Ω–æ):
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_connection():
    host = 'rc1b-uh7kdmcx67eomesf.mdb.yandexcloud.net'
    port = 6432
    db = 'playground_mle_20250530_d398f4a560'
    username = 'mle_20250530_d398f4a560' #mle_ro
    password = '83284b762ddc448d83912fd1d83a40fd'

    connection_str = f'postgresql://{username}:{password}@{host}:{port}/{db}'
    logger.info(f"üì° –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î: {connection_str}")
    
    engine = create_engine(connection_str, connect_args={'sslmode': 'require'})
    return engine

def get_clean_data() -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –ë–î –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ csv."""

    engine = create_connection()
    query = "SELECT * FROM real_estate_dataset_clean"

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –ë–î —á–µ—Ä–µ–∑ Pandas
    df = pd.read_sql(query, engine)
    logger.info(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫ –∏–∑ –ë–î: {len(df)}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –Ω—É–∂–Ω—ã–π CSV (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å ../../../data)
    data_dir = '/home/mle-user/mle_projects/mle-dvc/Progect1/mle-project-sprint-1-v001/part2_dvc/data'
    os.makedirs(data_dir, exist_ok=True)
    data_path = os.path.join(data_dir, 'clean_data.csv')
    df.to_csv(data_path, index=False)
    logger.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {data_path}")

    return df

def train_model(df: pd.DataFrame):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ –≤—ã–±–æ—Ä–∫–∏ –∏ –æ—Ü–µ–Ω–∫–æ–π."""
    target = 'price'
    X = df.drop(target, axis=1)
    y = df[target]

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    cat_features = X.select_dtypes(include='object')
    potential_binary_features = cat_features.nunique() == 2

    binary_cat_features = cat_features[potential_binary_features[potential_binary_features].index]
    other_cat_features = cat_features[potential_binary_features[~potential_binary_features].index]
    num_features = X.select_dtypes(include=['float64', 'int64'])

    preprocessor = ColumnTransformer(
        [
            ('binary', OneHotEncoder(drop='if_binary'), binary_cat_features.columns.tolist()),
            ('cat', CatBoostEncoder(return_df=False), other_cat_features.columns.tolist()),
            ('num', StandardScaler(), num_features.columns.tolist())
        ],
        remainder='drop',
        verbose_feature_names_out=False
    )

    model = CatBoostRegressor(silent=True)

    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])
    
    # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ train-–≤—ã–±–æ—Ä–∫–µ
    pipeline.fit(X_train, y_train)

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    preds = pipeline.predict(X_test)
    mae = mean_absolute_error(y_test, preds)
    rmse = mean_squared_error(y_test, preds, squared=False)
    r2 = r2_score(y_test, preds)

    logger.info(f"‚úÖ MAE –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {mae:.2f}")
    logger.info(f"‚úÖ RMSE –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {rmse:.2f}")
    logger.info(f"‚úÖ R2 –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {r2:.2f}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_dir = '/home/mle-user/mle_projects/mle-dvc/Progect1/mle-project-sprint-1-v001/part2_dvc/models'
    os.makedirs(model_dir, exist_ok=True)
    model_path = os.path.join(model_dir, 'price_predictor_model.pkl')
    joblib.dump(pipeline, model_path)
    logger.info(f"üì¶ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")

def main():
    logger.info("üöÄ –°–∫—Ä–∏–ø—Ç –Ω–∞—á–∞–ª —Ä–∞–±–æ—Ç—É. –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ.")
    df = get_clean_data()  # –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î
    logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö: {len(df)}")

    # –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    train_model(df)

    logger.info("üèÅ –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É.")

if __name__ == "__main__":
    main()