import pandas as pd
from sklearn.model_selection import KFold, cross_validate
from sklearn.metrics import make_scorer, mean_absolute_error, r2_score
import joblib
import json
import yaml
import os
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏
def evaluate_model():
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ params.yaml
    with open('params.yaml', 'r') as fd:
        params = yaml.safe_load(fd)

    target = params.get('target', 'price')
    input_path = params.get('input_data', 'data/initial_data.csv')
    model_path = params.get('model_output', 'models/price_predictor_model.pkl')

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏
    data = pd.read_csv(input_path)
    logger.info(f"üì• –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –∏–∑ {input_path}")

    if target not in data.columns:
        raise ValueError(f"‚ùå –¶–µ–ª–µ–≤–æ–π —Å—Ç–æ–ª–±–µ—Ü '{target}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö.")

    X = data.drop(columns=[target])
    y = data[target]

    model = joblib.load(model_path)
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    cv = KFold(n_splits=5, shuffle=True, random_state=42)

    scoring = {
        'mae': make_scorer(mean_absolute_error),
        'r2': make_scorer(r2_score)
    }

    logger.info("üîÑ –ù–∞—á–∞–ª–æ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏...")
    cv_results = cross_validate(
        model,
        X,
        y,
        cv=cv,
        scoring=scoring,
        n_jobs=-1
    )

    avg_results = {
        'mae': round(cv_results['test_mae'].mean(), 3),
        'r2': round(cv_results['test_r2'].mean(), 3)
    }

    logger.info(f"üìä MAE: {avg_results['mae']}")
    logger.info(f"üìà R¬≤: {avg_results['r2']}")

    #  –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON
    OUT_DIR = 'Progect1/mle-project-sprint-1-v001/part2_dvc/cv_results'
    os.makedirs(OUT_DIR, exist_ok=True)

    with open(os.path.join(OUT_DIR, 'evaluation_results.json'), 'w') as fd:
        json.dump(avg_results, fd)

    logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {os.path.join(OUT_DIR, 'evaluation_results.json')}")


if __name__ == '__main__':
    evaluate_model()